\documentclass[fleqn, 10pt]{beamer}


\title[Econ-672]{Economics 672: Econometric Analysis II \\ Winter 2018}
\author[M. Cattaneo]{Matias Cattaneo \\
        Department of Economics \\
        University of Michigan \\
        Authored collaboratively by PhD entering class of 2017
        }
\date{January 16, 2018}

\usetheme{AnnArbor}
\usecolortheme{wolverine}

\setbeamertemplate{footline}
{%
  \begin{beamercolorbox}{section in head/foot}
	\hspace{1em} \insertshortauthor \hfill \insertshorttitle \hfill \insertdate \hspace{1cm} \insertframenumber /  \inserttotalframenumber \hspace{1em}
  \end{beamercolorbox}%
}




\usepackage{hyperref}
\usepackage{import}
\usepackage{caption}
\captionsetup{labelformat=empty,labelsep=none}
\usepackage{amsmath}
\setlength{\mathindent}{0.5 em}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{setspace}
%\usepackage{doc}
%\usepackage[caption=false]{subfig}


\usebeamercolor{structure}
\usebeamercolor{frametitle}
%\setbeamercolor{block title}{bg=\color{structure.fg}, fg=black}

\lstset{basicstyle= \ttfamily \scriptsize, frame=trbl, backgroundcolor=\color{frametitle.bg},rulecolor=\color{structure.fg}}
\usepackage{xcolor}

\def\lst{\lstinline}

\newcommand{\com}[1]{{\footnotesize\texttt{#1}}}

\hypersetup{
    bookmarks=true,                 % show bookmarks bar?
    unicode=false,                  % non-Latin characters in bookmarks
    pdftoolbar=true,                % show toolbar?
    pdfmenubar=true,                % show menu?
    pdffitwindow=true,              % window fit to page when opened
    pdfstartview={FitH},            % fits the width of the page to the window
    pdftitle={},                    % title
    pdfauthor={Jamie Fogel},        % author
    pdfnewwindow=true,              % links in new window
    colorlinks=true,                % false: boxed links; true: colored links
    linkcolor=blue,                 % color of internal links
    citecolor=black,                % color of links to bibliography
    filecolor=magenta,              % color of file links
    urlcolor=blue,                  % color of external links
    breaklinks=true
}

\AtBeginSection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[currentsection]
	\end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[plain]
    \titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Introduction}
	\begin{itemize}
		\item This lecture:
		\begin{itemize}
			\item Ordinary Least Squares
		\end{itemize}
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ordinary Least Squares}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Assumptions and setup}

    \begin{itemize}
		\item $\{(y_i,x'_i)':i=1,\dots,n\}$, $y_i \in \mathbb{R}$, $x_i \in \mathbb{R}^d$
		\begin{itemize}
			\item $y$ is a $n\times 1$ vector of outcomes of interest, $X$ is a $n\times d$ matrix; the $i^{th}$ row of which is $x'_i$
		\end{itemize}
		\item Postulate 2 basic assumptions:
		\begin{itemize}
			\item $\mathbb{E}(y|x)=x\beta_0$, where $\beta_0 \in \mathbb{R}^d$
			\item $\mathbb{V}(y|x)=\sigma^2_0 I_n $ (true under homoskedasticity and iid sampling)
			\item Later, add a third assumption: $Y|X\sim \mathcal{N}(x\beta_0,\sigma^2_0I_n) $
		\end{itemize}
	\end{itemize}
	
	With a model of the form $Y=X\beta_0+\varepsilon$, these conditions become: $\mathbb{E}(\varepsilon|X)=0 $ (A1), $\mathbb{V}(\varepsilon|X)=\sigma^2_0 I_n $ (A2), and $\varepsilon|X \sim \mathcal{N}(0,\sigma^2_0I_n) $ (A3).
	
\end{frame}

\begin{frame}{OLS Normal Equations}
	We want to solve the problem $\min_{\beta\in \mathbb{R}^d}||y-X\beta||^2 = \min_{\beta\in \mathbb{R}^d}\sum^n_{i=1}(y_i-x'_i\beta)^2$. Postulate that there exists at least one solution to this: call it $\hat{\beta}$.
	\begin{align*}
	||y-X\beta||^2&= ||y-X\hat{\beta}+X\hat{\beta}-X\beta||^2 \\
	&=(y-X\hat{\beta}+X\hat{\beta}-X\beta)'(y-X\hat{\beta}+X\hat{\beta}-X\beta) \\
	&= (y-X\hat{\beta})'(y-X\hat{\beta})+2(y-X\hat{\beta})(X\hat{\beta}-X\beta)+(X\hat{\beta}-X\beta)'(X\hat{\beta}-X\beta) \\
	&=||y-X\hat{\beta}||^2+2(y-X\hat{\beta})'X(\hat{\beta}-\beta)+||X(\hat{\beta}-\beta)||^2
	\end{align*}
	
	Setting $\hat{\beta}$ such that the middle term $=0$ implies that we choose $\hat{\beta}$ such that $(y-X\hat{\beta})'X=0 $. This implies the \textbf{least squares normal equations}:

	\begin{align*}
	\hat{\beta}&=(X'X)^-X'y
	\end{align*}
	
	\begin{itemize}
		\item $y-X\hat{\beta}=0$ only in the case of perfect prediction. In general this will not happen; but if $y$ is a vector that can be generated perfectly by the column space of $X$, then it will hold.
	\end{itemize}
\end{frame}

\begin{frame}{Projection and Annihilator Matrices}
	
	\begin{itemize}
		\item By A1, $\hat{\beta}=(X'X)^-X'y \Rightarrow \hat{y}=X(X'X)^-X'y$
		\begin{itemize}
			\item $\hat{y}$ is the closest approximation to $y$ that lives in the space generated by the column vectors of $X$.
		\end{itemize}
		\item $P_x = X(X'X)^-X'$ is the \textbf{projection matrix}. Properties include:
		\begin{itemize}
			\item $X$ is invariant under $P_x$: $P_x X=X$
			\item Symmetry: $P'_x=P_x $
			\item Idempotence: $P_x P_x = P_x $ 
		\end{itemize}
		\item $M_x = I-P_x$ projects on the orthogonal complement of $X$. Properties include:
		\begin{itemize}
			\item $M_x X=0$
			\item $M_x P_x=0$
			\item $M'_x = M_x $
			\item $M_xM_x=M_x$
		\end{itemize}
		\item By definition of $\hat\varepsilon$ and $\hat{y}$, $\hat\varepsilon = y-\hat{y}=(I-P_x)y $
		\begin{itemize}
			\item The residuals, $\hat{\varepsilon}$, are the projection of $y$ onto the orthogonal complement of $X$. 
			\item $\hat{\varepsilon}'\hat{y}=0 $ and $X'\hat{\varepsilon}=0$
			\item Any vector $v$ can be written as $v=P_v v + M_v v$
		\end{itemize}	
	\end{itemize}
	
\end{frame}

\begin{frame}{Gauss-Markov Theorem}
If the following two assumptions hold:		
	\begin{itemize}
		\item $\mathbb{E}(Y|X)=X\beta_0$
		\item $\mathbb{V}(Y|X)=\sigma^2_0I_n$
	\end{itemize}
Then $\hat{\beta}=(X'X)^-X'y$ is the best linear (conditionally) unbiased estimator of $\beta_0$. $\mathbb{V}(\hat{\beta}|X)$ is no larger than that of any other (conditionally) unbiased estimator of $\beta_0$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Next Lecture: Topics and Readings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Next Class Topics and Readings}
	
	\begin{itemize}
		\item \underline{Problem Set \#}
		\item Next topic: blah blah blah
		\item \textbf{Readings:}
		\begin{enumerate}
			\item books
			\item more books
		\end{enumerate}
	\end{itemize}
	
\end{frame}


\end{document}
